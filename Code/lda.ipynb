{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c64ff27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in c:\\programdata\\anaconda3\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: funcy in c:\\programdata\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.17)\n",
      "Requirement already satisfied: gensim in c:\\programdata\\anaconda3\\lib\\site-packages (from pyLDAvis) (4.1.2)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.1.0)\n",
      "Requirement already satisfied: sklearn in c:\\programdata\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.0)\n",
      "Requirement already satisfied: future in c:\\programdata\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.7.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.0.2)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from pyLDAvis) (58.0.4)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.3.4)\n",
      "Requirement already satisfied: numexpr in c:\\programdata\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (0.29.23)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->pyLDAvis) (2.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sebgha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib_inline\\config.py:66: DeprecationWarning: InlineBackend._figure_formats_changed is deprecated in traitlets 4.1: use @observe and @unobserve instead.\n",
      "  def _figure_formats_changed(self, name, old, new):\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gensim\n",
    "import pyLDAvis\n",
    "!pip install pyLDAvis\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import nltk; nltk.download('stopwords')\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import re\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import seaborn as sns\n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff98e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rev_train.pkl', 'rb') as f:\n",
    "    rev_train = pickle.load(f)\n",
    "with open('rev_test.pkl', 'rb') as f:\n",
    "    rev_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ffb6019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108356\n",
      "108356\n",
      "108356\n"
     ]
    }
   ],
   "source": [
    "print(len(rev_train))\n",
    "mask = (rev_train['text'].str.len() > 50) & (rev_train['text'].str.len() < 200)\n",
    "rev_train = rev_train.loc[mask]\n",
    "print(len(rev_train))\n",
    "print(len(rev_train.sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34298807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['come','order','try','go','get','make','drink','plate','dish','restaurant','place',\n",
    "                  'would','really','like','great','service','came','got'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c4ae091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_newline(series):\n",
    "    return [review.replace('\\n','') for review in series]\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "        \n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "def bigrams(words, bi_min=15, tri_min=10):\n",
    "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return bigram_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be437fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['come','order','try','go','get','make','drink','plate','dish','restaurant','place','would','really','like','great','service','came','got']) \n",
    "def remove_stopwords(texts):\n",
    "    out = [[word for word in simple_preprocess(str(doc))\n",
    "            if word not in stop_words]\n",
    "            for doc in texts]\n",
    "    return out\n",
    "def bigrams(words, bi_min=15, tri_min=10):\n",
    "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return bigram_mod\n",
    "def get_corpus(df):\n",
    "    df['text'] = strip_newline(df.text)\n",
    "    words = list(sent_to_words(df.text))\n",
    "    words = remove_stopwords(words)\n",
    "    bigram_mod = bigrams(words)\n",
    "    bigram = [bigram_mod[review] for review in words]\n",
    "    id2word = gensim.corpora.Dictionary(bigram)\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
    "    id2word.compactify()\n",
    "    corpus = [id2word.doc2bow(text) for text in bigram]\n",
    "    \n",
    "    return corpus, id2word, bigram\n",
    "train_corpus, train_id2word, bigram_train = get_corpus(rev_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "376eae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_id2word.pkl', 'wb') as f:\n",
    "    pickle.dump(train_id2word, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3361a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations 20 f1 0.941669006713808\n",
      "iterations 20 f1 0.9426714042706458\n",
      "iterations 20 f1 0.9419760091839935\n",
      "iterations 20 f1 0.9419785403300832\n",
      "iterations 20 f1 0.9421606370926321\n",
      "iterations 50 f1 0.9419624643002855\n",
      "iterations 50 f1 0.9437714738165972\n",
      "iterations 50 f1 0.9429675953830589\n",
      "iterations 50 f1 0.942660856214878\n",
      "iterations 50 f1 0.9429209555033035\n",
      "iterations 80 f1 0.9403669724770641\n",
      "iterations 80 f1 0.9423697076814432\n",
      "iterations 80 f1 0.9419619620637066\n",
      "iterations 80 f1 0.9415661670691788\n",
      "iterations 80 f1 0.9416727042835781\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gensim\n",
    "maxf1=0\n",
    "for iterations in range(20,100,30):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "\n",
    "        lda_train = gensim.models.ldamulticore.LdaMulticore(corpus=train_corpus,num_topics=20, iterations=iterations,id2word=train_id2word,chunksize=100,workers=7, passes=50,eval_every = 1,per_word_topics=True)\n",
    "        train_vecs = []\n",
    "        for i in range(len(rev_train)):\n",
    "            top_topics = (lda_train.get_document_topics(train_corpus[i],minimum_probability=0.0))\n",
    "            topic_vec = [top_topics[i][1] for i in range(20)]\n",
    "            topic_vec.extend([len(rev_train.iloc[i].text)])\n",
    "            train_vecs.append(topic_vec)\n",
    "        X = np.array(train_vecs)\n",
    "        y = np.array(rev_train.sentiment)\n",
    "\n",
    "        kf = KFold(5, shuffle=True, random_state=42)\n",
    "        cv_lr_f1, cv_lrsgd_f1, cv_svcsgd_f1,  = [], [], []\n",
    "\n",
    "        for train_ind, val_ind in kf.split(X, y):\n",
    "                        # Assign CV IDX\n",
    "            X_train, y_train = X[train_ind], y[train_ind]\n",
    "            X_val, y_val = X[val_ind], y[val_ind]\n",
    "\n",
    "                        # Scale Data\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scale = scaler.fit_transform(X_train)\n",
    "            X_val_scale = scaler.transform(X_val)\n",
    "\n",
    "                        # Logisitic Regression\n",
    "            lr = RandomForestClassifier().fit(X_train_scale, y_train)\n",
    "\n",
    "            y_pred = lr.predict(X_val_scale)\n",
    "            cv_lr_f1.append(f1_score(y_val, y_pred, average='binary'))\n",
    "            f1s=np.mean(cv_lr_f1)\n",
    "            print(\"iterations\",iterations, \"f1\",f1s)\n",
    "            #if f1s>maxf1:\n",
    "                #lda_train.save('lda_train.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc3a3bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha 0.005 f1 0.9408964923278829\n",
      "alpha 0.05 f1 0.9388320129741189\n",
      "alpha 0.5 f1 0.9490368380845944\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gensim\n",
    "maxf1=0\n",
    "a=[0.005,0.05,0.5]\n",
    "for alpha in a:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "\n",
    "        lda_train = gensim.models.ldamulticore.LdaMulticore(corpus=train_corpus,num_topics=20, alpha=alpha,iterations=50,id2word=train_id2word,chunksize=100,workers=7, passes=50,eval_every = 1,per_word_topics=True)\n",
    "        train_vecs = []\n",
    "        for i in range(len(rev_train)):\n",
    "            top_topics = (lda_train.get_document_topics(train_corpus[i],minimum_probability=0.0))\n",
    "            topic_vec = [top_topics[i][1] for i in range(20)]\n",
    "            topic_vec.extend([len(rev_train.iloc[i].text)])\n",
    "            train_vecs.append(topic_vec)\n",
    "        X = np.array(train_vecs)\n",
    "        y = np.array(rev_train.sentiment)\n",
    "\n",
    "        kf = KFold(5, shuffle=True, random_state=42)\n",
    "        cv_lr_f1, cv_lrsgd_f1, cv_svcsgd_f1,  = [], [], []\n",
    "\n",
    "        for train_ind, val_ind in kf.split(X, y):\n",
    "                        # Assign CV IDX\n",
    "            X_train, y_train = X[train_ind], y[train_ind]\n",
    "            X_val, y_val = X[val_ind], y[val_ind]\n",
    "\n",
    "                        # Scale Data\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scale = scaler.fit_transform(X_train)\n",
    "            X_val_scale = scaler.transform(X_val)\n",
    "\n",
    "                        # Logisitic Regression\n",
    "            lr = RandomForestClassifier().fit(X_train_scale, y_train)\n",
    "\n",
    "            y_pred = lr.predict(X_val_scale)\n",
    "            cv_lr_f1.append(f1_score(y_val, y_pred, average='binary'))\n",
    "        f1s=np.mean(cv_lr_f1)\n",
    "        print(\"alpha\",alpha, \"f1\",f1s)\n",
    "            #if f1s>maxf1:\n",
    "                #lda_train.save('lda_train.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a979b603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta 0.005 f1 0.9406986581476027\n",
      "eta 0.05 f1 0.9444861848108724\n",
      "eta 0.5 f1 0.9600172805492122\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gensim\n",
    "maxf1=0\n",
    "b=[0.005,0.05,0.5]\n",
    "for eta in b:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "\n",
    "        lda_train = gensim.models.ldamulticore.LdaMulticore(corpus=train_corpus,num_topics=20, eta=eta,iterations=50,id2word=train_id2word,chunksize=100,workers=7, passes=50,eval_every = 1,per_word_topics=True)\n",
    "        train_vecs = []\n",
    "        for i in range(len(rev_train)):\n",
    "            top_topics = (lda_train.get_document_topics(train_corpus[i],minimum_probability=0.0))\n",
    "            topic_vec = [top_topics[i][1] for i in range(20)]\n",
    "            topic_vec.extend([len(rev_train.iloc[i].text)])\n",
    "            train_vecs.append(topic_vec)\n",
    "        X = np.array(train_vecs)\n",
    "        y = np.array(rev_train.sentiment)\n",
    "\n",
    "        kf = KFold(5, shuffle=True, random_state=42)\n",
    "        cv_lr_f1, cv_lrsgd_f1, cv_svcsgd_f1,  = [], [], []\n",
    "\n",
    "        for train_ind, val_ind in kf.split(X, y):\n",
    "                        # Assign CV IDX\n",
    "            X_train, y_train = X[train_ind], y[train_ind]\n",
    "            X_val, y_val = X[val_ind], y[val_ind]\n",
    "\n",
    "                        # Scale Data\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scale = scaler.fit_transform(X_train)\n",
    "            X_val_scale = scaler.transform(X_val)\n",
    "\n",
    "                        # Logisitic Regression\n",
    "            lr = RandomForestClassifier().fit(X_train_scale, y_train)\n",
    "\n",
    "            y_pred = lr.predict(X_val_scale)\n",
    "            cv_lr_f1.append(f1_score(y_val, y_pred, average='binary'))\n",
    "        f1s=np.mean(cv_lr_f1)\n",
    "        print(\"eta\",eta, \"f1\",f1s)\n",
    "            #if f1s>maxf1:\n",
    "                #lda_train.save('lda_train.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "911a82e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_topics 10 f1 0.9561402992825965\n",
      "num_topics 20 f1 0.9588342730652364\n",
      "num_topics 30 f1 0.9592046446853185\n",
      "num_topics 70 f1 0.9615967150986364\n",
      "num_topics 100 f1 0.9630671139628937\n",
      "num_topics 150 f1 0.9603605112088595\n",
      "num_topics 250 f1 0.9629133808307259\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gensim\n",
    "maxf1=0\n",
    "a=[10,20,30,70,100,150,250]\n",
    "for num_topics in a:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "\n",
    "        lda_train = gensim.models.ldamulticore.LdaMulticore(corpus=train_corpus,num_topics=num_topics, alpha=0.5,eta=0.5,iterations=50,id2word=train_id2word,chunksize=100,workers=7, passes=50,eval_every = 1,per_word_topics=True)\n",
    "        train_vecs = []\n",
    "        for i in range(len(rev_train)):\n",
    "            top_topics = (lda_train.get_document_topics(train_corpus[i],minimum_probability=0.0))\n",
    "            topic_vec = [top_topics[i][1] for i in range(num_topics)]\n",
    "            topic_vec.extend([len(rev_train.iloc[i].text)])\n",
    "            train_vecs.append(topic_vec)\n",
    "        X = np.array(train_vecs)\n",
    "        y = np.array(rev_train.sentiment)\n",
    "\n",
    "        kf = KFold(5, shuffle=True, random_state=42)\n",
    "        cv_lr_f1, cv_lrsgd_f1, cv_svcsgd_f1,  = [], [], []\n",
    "\n",
    "        for train_ind, val_ind in kf.split(X, y):\n",
    "                        # Assign CV IDX\n",
    "            X_train, y_train = X[train_ind], y[train_ind]\n",
    "            X_val, y_val = X[val_ind], y[val_ind]\n",
    "\n",
    "                        # Scale Data\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scale = scaler.fit_transform(X_train)\n",
    "            X_val_scale = scaler.transform(X_val)\n",
    "\n",
    "                        # Logisitic Regression\n",
    "            lr = RandomForestClassifier().fit(X_train_scale, y_train)\n",
    "\n",
    "            y_pred = lr.predict(X_val_scale)\n",
    "            cv_lr_f1.append(f1_score(y_val, y_pred, average='binary'))\n",
    "        f1s=np.mean(cv_lr_f1)\n",
    "        print(\"num_topics\",num_topics, \"f1\",f1s)\n",
    "        if f1s>maxf1:\n",
    "            lda_train.save('lda_train.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5455fea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_topics 250 f1 0.9633162588279239\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10200/203357654.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mlda_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mldamulticore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLdaMulticore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_corpus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0meta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_id2word\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0meval_every\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mtrain_vecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrev_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamulticore.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, workers, chunksize, passes, batch, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, random_state, minimum_probability, minimum_phi_value, per_word_topics, dtype)\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"auto-tuning alpha not implemented in LdaMulticore; use plain LdaModel.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m         super(LdaMulticore, self).__init__(\n\u001b[0m\u001b[0;32m    187\u001b[0m             \u001b[0mcorpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mid2word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m             self.add_lifecycle_event(\n\u001b[0;32m    522\u001b[0m                 \u001b[1;34m\"created\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamulticore.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, corpus, chunks_as_numpy)\u001b[0m\n\u001b[0;32m    309\u001b[0m                         \u001b[0mprocess_result_queue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m                 \u001b[0mprocess_result_queue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m             \u001b[1;31m# endfor single corpus pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamulticore.py\u001b[0m in \u001b[0;36mprocess_result_queue\u001b[1;34m(force)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mforce\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmerged_new\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mqueue_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumdocs\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mupdateafter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpass_\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m                 \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0meval_every\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mforce\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_updates\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mupdateafter\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0meval_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mdo_mstep\u001b[1;34m(self, rho, other, extra_pass)\u001b[0m\n\u001b[0;32m   1064\u001b[0m         \u001b[1;31m# the topics change through this update, to assess convergence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m         \u001b[0mprevious_Elogbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_Elogbeta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1066\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrho\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1067\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m         \u001b[0mcurrent_Elogbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_Elogbeta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mblend\u001b[1;34m(self, rhot, other, targetsize)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0mscale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtargetsize\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumdocs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msstats\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrhot\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[1;31m# stretch the incoming n*phi counts to target size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gensim\n",
    "maxf1=0\n",
    "a=[250,500]\n",
    "for num_topics in a:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "\n",
    "        lda_train = gensim.models.ldamulticore.LdaMulticore(corpus=train_corpus,num_topics=num_topics, alpha=0.5,eta=0.5,iterations=50,id2word=train_id2word,chunksize=100,workers=7, passes=50,eval_every = 1,per_word_topics=True)\n",
    "        train_vecs = []\n",
    "        for i in range(len(rev_train)):\n",
    "            top_topics = (lda_train.get_document_topics(train_corpus[i],minimum_probability=0.0))\n",
    "            topic_vec = [top_topics[i][1] for i in range(num_topics)]\n",
    "            topic_vec.extend([len(rev_train.iloc[i].text)])\n",
    "            train_vecs.append(topic_vec)\n",
    "        X = np.array(train_vecs)\n",
    "        y = np.array(rev_train.sentiment)\n",
    "\n",
    "        kf = KFold(5, shuffle=True, random_state=42)\n",
    "        cv_lr_f1, cv_lrsgd_f1, cv_svcsgd_f1,  = [], [], []\n",
    "\n",
    "        for train_ind, val_ind in kf.split(X, y):\n",
    "                        # Assign CV IDX\n",
    "            X_train, y_train = X[train_ind], y[train_ind]\n",
    "            X_val, y_val = X[val_ind], y[val_ind]\n",
    "\n",
    "                        # Scale Data\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scale = scaler.fit_transform(X_train)\n",
    "            X_val_scale = scaler.transform(X_val)\n",
    "\n",
    "                        # Logisitic Regression\n",
    "            lr = RandomForestClassifier().fit(X_train_scale, y_train)\n",
    "\n",
    "            y_pred = lr.predict(X_val_scale)\n",
    "            cv_lr_f1.append(f1_score(y_val, y_pred, average='binary'))\n",
    "        f1s=np.mean(cv_lr_f1)\n",
    "        print(\"num_topics\",num_topics, \"f1\",f1s)\n",
    "        if f1s>maxf1:\n",
    "            lda_train.save('lda_train.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53da4f46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
